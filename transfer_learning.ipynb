{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Import and configure libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.io as tf_io\n",
        "import tensorflow.strings as tf_strings\n",
        "import time\n",
        "\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Define model constants**\n",
        "\n",
        "Can be updated as needed, specifically:\n",
        "* `BATCH_SIZE`, to accomodate the memory restrictions of your machine\n",
        "* `EPOCHS`, to train for longer/shorter\n",
        "* `SEQ_LEN`, to change the context window of the model\n",
        "* `DATASET_FILE`, to use a dataset stored elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# General\n",
        "GPT2_PRESET = \"gpt2_base_en\"\n",
        "\n",
        "# LoRA\n",
        "RANK = 4\n",
        "ALPHA = 32.0\n",
        "\n",
        "# Data\n",
        "BATCH_SIZE = 2 # Batch size we train on\n",
        "SEQ_LEN = 512  # Length of training sequences, in tokens. AKA the context size\n",
        "NUM_BATCHES = 2  # Number of batches to train on\n",
        "\n",
        "# Training\n",
        "EPOCHS = 1\n",
        "\n",
        "# File names\n",
        "DATASET_FILE = \"RecipeNLG/RecipeNLG_dataset.csv\" # where the training data is stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def csv_row_to_json(row):\n",
        "    row = tf_io.decode_csv(records=row, record_defaults=[tf.constant([],dtype=tf.string)] * 7)\n",
        "\n",
        "    title = row[1]\n",
        "    ingredients = row[2]\n",
        "    directions = row[3]\n",
        "    ner = row[6]\n",
        "\n",
        "    # preserve the semi-structured nature of the dataset\n",
        "    return tf_strings.join([\n",
        "        '{\"ner\": ', ner, ', ',\n",
        "        '\"title\": \"', title, '\", ',\n",
        "        '\"ingredients\": ', ingredients, ', ',\n",
        "        '\"directions\": ', directions, '}',\n",
        "    ])\n",
        "\n",
        "\n",
        "dataset = (\n",
        "    tf_data.TextLineDataset(DATASET_FILE) # load the csv file line by line\n",
        "    .skip(1) # skip the header row\n",
        "    .shuffle(buffer_size=256) # store 256 shuffled records in memory at a time before reshuffling and refetching\n",
        "    .map(lambda row: csv_row_to_json(row)) # map each row of the csv to a json-formatted string\n",
        "    # .ignore_errors() # ignore any errors thrown by misformatted rows of the csv\n",
        "    .apply(tf.data.experimental.ignore_errors()) \n",
        "    .batch(BATCH_SIZE) # batch the dataset to train on multiple records at once\n",
        "    .take(NUM_BATCHES) # only train on the first NUM_BATCHES batches\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, input_text, max_length=200):\n",
        "    start = time.time()\n",
        "\n",
        "    output = model.generate(input_text, max_length=max_length)\n",
        "    print(\"\\nOutput:\")\n",
        "    print(output)\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Total Time Elapsed: {end - start:.2f}s\")\n",
        "    \n",
        "\n",
        "def get_optimizer_and_loss():\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        epsilon=1e-6,\n",
        "        global_clipnorm=1.0,  # Gradient clipping.\n",
        "    )\n",
        "    # Exclude layernorm and bias terms from weight decay.\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
        "\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return optimizer, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Create a LoRA layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class LoraLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        rank=8,\n",
        "        alpha=32,\n",
        "        trainable=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # We want to keep the name of this layer the same as the original\n",
        "        # dense layer.\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self._num_heads = original_layer_config[\"output_shape\"][-2]\n",
        "        self._hidden_dim = self._num_heads * original_layer_config[\"output_shape\"][-1]\n",
        "\n",
        "        # Layers.\n",
        "\n",
        "        # Original dense layer.\n",
        "        self.original_layer = original_layer\n",
        "        # No matter whether we are training the model or are in inference mode,\n",
        "        # this layer should be frozen.\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "        # LoRA dense layers.\n",
        "        self.A = keras.layers.Dense(\n",
        "            units=rank,\n",
        "            use_bias=False,\n",
        "            # Note: the original paper mentions that normal distribution was\n",
        "            # used for initialization. However, the official LoRA implementation\n",
        "            # uses \"Kaiming/He Initialization\".\n",
        "            kernel_initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_A\",\n",
        "        )\n",
        "        # B has the same `equation` and `output_shape` as the original layer.\n",
        "        # `equation = abc,cde->abde`, where `a`: batch size, `b`: sequence\n",
        "        # length, `c`: `hidden_dim`, `d`: `num_heads`,\n",
        "        # `e`: `hidden_dim//num_heads`. The only difference is that in layer `B`,\n",
        "        # `c` represents `rank`.\n",
        "        self.B = keras.layers.EinsumDense(\n",
        "            equation=original_layer_config[\"equation\"],\n",
        "            output_shape=original_layer_config[\"output_shape\"],\n",
        "            kernel_initializer=\"zeros\",\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_B\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_output = self.original_layer(inputs)\n",
        "        if self.trainable:\n",
        "            # If we are fine-tuning the model, we will add LoRA layers' output\n",
        "            # to the original layer's output.\n",
        "            lora_output = self.B(self.A(inputs)) * self._scale\n",
        "            return original_output + lora_output\n",
        "\n",
        "        # If we are in inference mode, we \"merge\" the LoRA layers' weights into\n",
        "        # the original layer's weights - more on this in the text generation\n",
        "        # section!\n",
        "        return original_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Load pre-trained model and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "with strategy.scope():\n",
        "    lora_model = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "        \"gpt2_base_en\",\n",
        "        preprocessor=preprocessor,\n",
        "    )\n",
        "lora_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Inject LoRA into the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for layer_idx in range(lora_model.backbone.num_layers):\n",
        "    # Change query dense layer.\n",
        "    decoder_layer = lora_model.backbone.get_layer(f\"transformer_layer_{layer_idx}\")\n",
        "    self_attention_layer = decoder_layer._self_attention_layer\n",
        "    # Allow mutation to Keras layer state.\n",
        "    self_attention_layer._tracker.locked = False\n",
        "\n",
        "    # Change query dense layer.\n",
        "    self_attention_layer._query_dense = LoraLayer(\n",
        "        self_attention_layer._query_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "    # Change value dense layer.\n",
        "    self_attention_layer._value_dense = LoraLayer(\n",
        "        self_attention_layer._value_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# freeze non-LoRA layers\n",
        "for layer in lora_model._flatten_layers():\n",
        "    lst_of_sublayers = list(layer._flatten_layers())\n",
        "\n",
        "    if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
        "        if layer.name in [\"lora_A\", \"lora_B\"]:\n",
        "            layer.trainable = True\n",
        "        else:\n",
        "            layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzaLSew751ID"
      },
      "source": [
        "---\n",
        "**Finetune the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='transfer_learning/checkpoint_{epoch:02d}.keras',\n",
        "    save_best_only=False,\n",
        ")\n",
        "\n",
        "with strategy.scope():\n",
        "    optimizer, loss = get_optimizer_and_loss()\n",
        "\n",
        "    lora_model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        weighted_metrics=[\"accuracy\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_model.fit(\n",
        "    dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Merge weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for layer_idx in range(lora_model.backbone.num_layers):\n",
        "    self_attention_layer = lora_model.backbone.get_layer(\n",
        "        f\"transformer_layer_{layer_idx}\"\n",
        "    )._self_attention_layer\n",
        "\n",
        "    # Merge query dense layer.\n",
        "    query_lora_layer = self_attention_layer._query_dense\n",
        "\n",
        "    A_weights = query_lora_layer.A.kernel  # (768, 1) (a, b)\n",
        "    B_weights = query_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
        "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
        "    query_lora_layer.original_layer.kernel.assign_add(increment_weights)\n",
        "\n",
        "    # Merge value dense layer.\n",
        "    value_lora_layer = self_attention_layer._value_dense\n",
        "\n",
        "    A_weights = value_lora_layer.A.kernel  # (768, 1) (a, b)\n",
        "    B_weights = value_lora_layer.B.kernel  # (1, 12, 64) (b, c, d)\n",
        "    increment_weights = tf.einsum(\"ab,bcd->acd\", A_weights, B_weights) * (ALPHA / RANK)\n",
        "    value_lora_layer.original_layer.kernel.assign_add(increment_weights)\n",
        "\n",
        "    # Put back in place the original layers with updated weights\n",
        "    self_attention_layer._query_dense = query_lora_layer.original_layer\n",
        "    self_attention_layer._value_dense = value_lora_layer.original_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Generate text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_text(lora_model, '{\"ner\": ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Save the final model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_model.save(\"transfer_learning/final_model.keras\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "notebook9003bbc4a7",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 1025978,
          "sourceId": 1728879,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30698,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
