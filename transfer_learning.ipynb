{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Import and configure libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.io as tf_io\n",
        "import tensorflow.strings as tf_strings\n",
        "import time\n",
        "\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Define model constants**\n",
        "\n",
        "Can be updated as needed, specifically:\n",
        "* `BATCH_SIZE`, to accomodate the memory restrictions of your machine\n",
        "* `EPOCHS`, to train for longer/shorter\n",
        "* `SEQ_LEN`, to change the context window of the model\n",
        "* `DATASET_FILE`, to use a dataset stored elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# General\n",
        "GPT2_PRESET = \"gpt2_base_en\"\n",
        "\n",
        "# LoRA\n",
        "RANK = 4\n",
        "\n",
        "# Data\n",
        "BATCH_SIZE = 2 # Batch size we train on\n",
        "SEQ_LEN = 512  # Length of training sequences, in tokens. AKA the context size\n",
        "NUM_BATCHES = 2  # Number of batches to train on\n",
        "\n",
        "# Training\n",
        "EPOCHS = 1\n",
        "\n",
        "# File names\n",
        "DATASET_FILE = \"RecipeNLG/RecipeNLG_dataset.csv\" # where the training data is stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def csv_row_to_json(row):\n",
        "    row = tf_io.decode_csv(records=row, record_defaults=[tf.constant([],dtype=tf.string)] * 7)\n",
        "\n",
        "    title = row[1]\n",
        "    ingredients = row[2]\n",
        "    directions = row[3]\n",
        "    ner = row[6]\n",
        "\n",
        "    # preserve the semi-structured nature of the dataset\n",
        "    return tf_strings.join([\n",
        "        '{\"ner\": ', ner, ', ',\n",
        "        '\"title\": \"', title, '\", ',\n",
        "        '\"ingredients\": ', ingredients, ', ',\n",
        "        '\"directions\": ', directions, '}',\n",
        "    ])\n",
        "\n",
        "\n",
        "dataset = (\n",
        "    tf_data.TextLineDataset(DATASET_FILE) # load the csv file line by line\n",
        "    .skip(1) # skip the header row\n",
        "    .shuffle(buffer_size=256) # store 256 shuffled records in memory at a time before reshuffling and refetching\n",
        "    .map(lambda row: csv_row_to_json(row)) # map each row of the csv to a json-formatted string\n",
        "    .ignore_errors() # ignore any errors thrown by misformatted rows of the csv\n",
        "    .batch(BATCH_SIZE) # batch the dataset to train on multiple records at once\n",
        "    .take(NUM_BATCHES) # only train on the first NUM_BATCHES batches\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, input_text, max_length=200):\n",
        "    start = time.time()\n",
        "\n",
        "    output = model.generate(input_text, max_length=max_length)\n",
        "    print(\"\\nOutput:\")\n",
        "    print(output)\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"Total Time Elapsed: {end - start:.2f}s\")\n",
        "    \n",
        "\n",
        "def get_optimizer_and_loss():\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        epsilon=1e-6,\n",
        "        global_clipnorm=1.0,  # Gradient clipping.\n",
        "    )\n",
        "    # Exclude layernorm and bias terms from weight decay.\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
        "\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return optimizer, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Load pre-trained model and enable LoRA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    GPT2_PRESET,\n",
        "    sequence_length=SEQ_LEN,\n",
        ")\n",
        "\n",
        "with strategy.scope():\n",
        "    model = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "        GPT2_PRESET,\n",
        "        preprocessor=preprocessor,\n",
        "    )\n",
        "    model.backbone.enable_lora(rank=RANK)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzaLSew751ID"
      },
      "source": [
        "---\n",
        "**Finetune the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SaveLoRACheckpoint(keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        save_dir = \"transfer_learning\"\n",
        "        os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "        save_name = os.path.join(save_dir, f\"checkpoint_{epoch:02d}.lora.h5\")\n",
        "        print(f\"\\n\\nSaving checkpoint to {save_name}... \", end=\"\")\n",
        "        self.model.backbone.save_lora_weights(save_name)\n",
        "        print(\"Done\\n\")\n",
        "\n",
        "checkpoint_callback = SaveLoRACheckpoint()\n",
        "\n",
        "with strategy.scope():\n",
        "    optimizer, loss = get_optimizer_and_loss()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        weighted_metrics=[\"accuracy\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_model.fit(\n",
        "    dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_name = os.path.join(\"transfer_learning\", \"final_model.keras\")\n",
        "lora_model.save(save_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "notebook9003bbc4a7",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 1025978,
          "sourceId": 1728879,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30698,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
