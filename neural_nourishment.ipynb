{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nourishment\n",
    "\n",
    "This project draws on examples from the Keras tutorials [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and [GPT text generation from scratch with KerasNLP](https://keras.io/examples/generative/text_generation_gpt/), as well as the papers [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. and [\"Language Models are Few-Shot Listeners\"](https://arxiv.org/abs/2005.14165) by Brown et al.\n",
    "\n",
    "It uses [WordPiece Tokenization](https://research.google/blog/a-fast-wordpiece-tokenization-system/) and is trained on the [RecipeNLG dataset](https://www.kaggle.com/datasets/paultimothymooney/recipenlg) of 2,231,142 cooking recipes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import neccesary libraries and the Tokenizer class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "import tensorflow.io as tf_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64\n",
    "MIN_STRING_LEN = 512  # Strings shorter than this will be discarded\n",
    "SEQ_LEN = 128  # Length of training sequences, in tokens\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256\n",
    "FEED_FORWARD_DIM = 128\n",
    "NUM_HEADS = 3\n",
    "NUM_LAYERS = 2\n",
    "VOCAB_SIZE = 4096  # Limits parameters in model.\n",
    "\n",
    "# Training\n",
    "EPOCHS = 1\n",
    "TOKENIZER_TRAINING_SIZE = 256\n",
    "\n",
    "# Inference\n",
    "NUM_TOKENS_TO_GENERATE = 80\n",
    "\n",
    "# Special tokens\n",
    "START_OF_RECIPE = \"<|recipe_start|>\"\n",
    "END_OF_RECIPE = \"<|recipe_end|>\"\n",
    "PAD = \"<|pad|>\"\n",
    "OOV = \"<|oov|>\"\n",
    "SPECIAL_TOKENS = [PAD, START_OF_RECIPE, END_OF_RECIPE, OOV]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define the dataset as strings of full recipes**\n",
    "\n",
    "To keep training managable for a laptop, we load the dataset into a tensorflow dataset object. This allows us to load data into memory as needed, opposed to all at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_row_to_json(row):\n",
    "    row = tf_io.decode_csv(records=row, record_defaults=[tf.constant([],dtype=tf.string)] * 7)\n",
    "    \n",
    "    title = row[1]\n",
    "    ingredients = row[2]\n",
    "    directions = row[3]\n",
    "    ner = row[6]\n",
    "\n",
    "    return tf_strings.join([\n",
    "        '{\"ner\": ', ner, ', ',\n",
    "        '\"title\": \"', title, '\", ',\n",
    "        '\"ingredients\": ', ingredients, ', ',\n",
    "        '\"directions\": ', directions, '}',\n",
    "    ])\n",
    "\n",
    "dataset = (\n",
    "    tf_data.TextLineDataset(\"RecipeNLG/RecipeNLG_dataset.csv\") # load the csv file line by line\n",
    "    .skip(1) # skip the header row\n",
    "    .shuffle(buffer_size=256) # store 256 shuffled records in memory at a time before reshuffling and refetching\n",
    "    .map(lambda row: csv_row_to_json(row)) # map each row of the csv to a jsonified recipe\n",
    "    .apply(tf.data.experimental.ignore_errors()) # ignore any errors in the csv file\n",
    "    .batch(BATCH_SIZE) # batch the dataset to train on multiple records at once\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Train the BPE tokenizer**\n",
    "\n",
    "On a subset of our data, we train a WordPiece tokenizer. Special tokens are used for denoting the beginning and end of recipes. Once trained, we can use the Keras `WordPieceTokenizer` to tokenize our tensors within the `tf.data` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 21:52:25.271213: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    data=dataset.take(TOKENIZER_TRAINING_SIZE),\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    reserved_tokens=SPECIAL_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    special_tokens_in_strings=True,\n",
    "    special_tokens=SPECIAL_TOKENS,\n",
    "    oov_token=OOV,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Tokenize the dataset**\n",
    "\n",
    "Start and end tokens get added, then recipes are tokenized and prepared for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(START_OF_RECIPE),\n",
    "    end_value=tokenizer.token_to_id(END_OF_RECIPE),\n",
    "    pad_value=tokenizer.token_to_id(PAD),\n",
    ")\n",
    "\n",
    "def preprocess(recipe_batch):\n",
    "    outputs = tokenizer(recipe_batch)\n",
    "    features = packer(outputs)\n",
    "    labels = outputs\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "    # token embedding layer\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=VOCAB_SIZE,\n",
    "        sequence_length=SEQ_LEN,\n",
    "        embedding_dim=EMBED_DIM,\n",
    "        mask_zero=True,\n",
    "    )\n",
    "\n",
    "    # transformer decoders\n",
    "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "    )\n",
    "\n",
    "    # output layer\n",
    "    output_layer = keras.layers.Dense(VOCAB_SIZE)\n",
    "\n",
    "    # build the model\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(NUM_LAYERS): x = decoder_layer(x)\n",
    "    outputs = output_layer(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=loss_function, metrics=[perplexity])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, None, 256)   1081344     ['input_1[0][0]']                \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   329085      ['token_and_position_embedding[0]\n",
      " erDecoder)                                                      [0]',                            \n",
      "                                                                  'transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 4096)   1052672     ['transformer_decoder[1][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,463,101\n",
      "Trainable params: 2,463,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Fit the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ModelCheckpoint(\"save_at_{epoch}.keras\"),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
