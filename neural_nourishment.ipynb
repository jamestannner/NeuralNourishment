{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import neccesary libraries and the Tokenizer class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "\n",
    "from tokenizer import Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define the dataset as strings of full recipes**\n",
    "\n",
    "To keep training managable for a laptop, we load the dataset into a tensorflow dataset object. This allows us to load data into memory as needed, opposed to all at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_OF_RECIPE = \"<|recipe_start|>\"\n",
    "END_OF_RECIPE = \"<|recipe_end|>\"\n",
    "\n",
    "@tf.py_function(Tout=tf.string)\n",
    "def parse_csv_row(row):\n",
    "    row = tf_strings.as_string(row)\n",
    "    row_values = next(csv.reader([row.numpy().decode('utf-8')]))\n",
    "\n",
    "    ner = eval(row_values[6])\n",
    "    title = row_values[1]\n",
    "    ingredients = eval(row_values[2])\n",
    "    directions = eval(row_values[3])\n",
    "\n",
    "    stringified_recipe = json.dumps({\n",
    "        'ner': ner,\n",
    "        'title': title,\n",
    "        'ingredients': ingredients,\n",
    "        'directions': directions,\n",
    "    })\n",
    "\n",
    "    return START_OF_RECIPE + stringified_recipe + END_OF_RECIPE\n",
    "\n",
    "# load in the csv file line by line\n",
    "dataset = tf_data.TextLineDataset(\"RecipeNLG/RecipeNLG_dataset.csv\")\n",
    " # skip the header row\n",
    "dataset = dataset.skip(1)\n",
    "# shuffles the ordering of the dataset. Stores 256 shuffled records in memory at a time before reshuffling\n",
    "dataset = dataset.shuffle(buffer_size=256)\n",
    "# map each row of the csv to a stringified recipe\n",
    "dataset = dataset.map(lambda row: parse_csv_row(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Train the BPE tokenizer**\n",
    "\n",
    "On a subset of our data, we train a custom byte-pair-encoding tokenizer. Special tokens are used for denoting the beginning and end of recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "VOCAB_SIZE = 2048\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    START_OF_RECIPE: VOCAB_SIZE - 1,\n",
    "    END_OF_RECIPE: VOCAB_SIZE - 2,\n",
    "}\n",
    "\n",
    "tokenizer_dataset = \"\"\n",
    "limit = 250\n",
    "for recipe in dataset:\n",
    "    if limit == 0: break\n",
    "    tokenizer_dataset += recipe.numpy().decode('utf-8')\n",
    "    limit -= 1\n",
    "\n",
    "tokenizer.register_special_tokens(SPECIAL_TOKENS)\n",
    "tokenizer.train(tokenizer_dataset, VOCAB_SIZE - len(SPECIAL_TOKENS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Tokenize the dataset**\n",
    "\n",
    "Recipes are batched, tokenized representation, and prepared for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 # defines the number of recipes we will process at a time\n",
    "MAX_LEN = 4096  # Max sequence size\n",
    "\n",
    "dataset = dataset.batch(batch_size=BATCH_SIZE) \n",
    "\n",
    "@tf.py_function(Tout=(tf.int32, tf.int32))\n",
    "def prepare_model_inputs(batch_of_recipes):\n",
    "    tokens = []\n",
    "    \n",
    "    for batch in batch_of_recipes:\n",
    "        recipe = tf_strings.as_string(batch) # convert the tensor to a string\n",
    "        tokenized_recipe = tokenizer.encode(recipe.numpy().decode('utf-8')) # tokenize the string\n",
    "        tokens.append(tokenized_recipe)\n",
    "        \n",
    "    X, y = [], []\n",
    "    for t in tokens:\n",
    "        zeros = [0] * (MAX_LEN - len(t)) # pad shorter token sequences with zeros to ensure uniform size\n",
    "        X.append(t[:-1] + zeros)\n",
    "        y.append(t[1:] + zeros)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "dataset = dataset.map(prepare_model_inputs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Implement a TransformerBlock layer and a TokenAndPositionEmbedding layer.**\n",
    "\n",
    "Based on the examples from [this Keras tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = ops.arange(n_dest)[:, None]\n",
    "    j = ops.arange(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = ops.cast(m, dtype)\n",
    "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = ops.concatenate(\n",
    "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return ops.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(0, maxlen, 1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define the model architecture and hyperparameters**\n",
    "\n",
    "Again, based on the examples from [this Keras tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 256 # Embedding size for each token\n",
    "NUM_ATTENTION_HEADS = 2 # Number of attention heads\n",
    "FEED_FORWARD_SIZE = 256 # Feed forward size in each transformer block\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(MAX_LEN,), dtype=\"int32\")\n",
    "    embedding_layer = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(EMBEDDING_SIZE, NUM_ATTENTION_HEADS, FEED_FORWARD_SIZE)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(VOCAB_SIZE)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\",\n",
    "        loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define our RecipeGenerator class**\n",
    "\n",
    "Inspired by the example in [the Keras tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) but adapted to fit this use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate a recipe from our trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input \n",
    "\n",
    "    Arguments:\n",
    "        start_tokens: List of integers, the tokens for the starting prompt.\n",
    "        top_k: Integer, sample from the `top_k` token predictions. Defaults to 10.\n",
    "        print_every: Integer, print after this many epochs. Defaults to 1.\n",
    "        max_tokens: Integer, the maximum number of tokens to be generated after prompt. \n",
    "            Generation will end early when we reach the `<|recipe_end|>` token. Defaults to 300.\n",
    "    \"\"\"\n",
    "    def __init__(self, start_tokens, top_k=10, print_every=1, max_tokens=300):\n",
    "        self.start_tokens = start_tokens\n",
    "        self.top_k = top_k\n",
    "        self.print_every = print_every\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, idxs = ops.top_k(logits, k=self.top_k, sorted=True)\n",
    "        idsx = np.asarray(idxs).astype(\"int32\")\n",
    "        predictions = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
    "        predictions = np.asarray(predictions).astype(\"float32\")\n",
    "        return np.random.choice(idxs, p=predictions)\n",
    "    \n",
    "    def on_epoch_end(self, epoch):\n",
    "        if (epoch + 1) % self.print_every != 0: return\n",
    "        \n",
    "        start_tokens = [token for token in self.start_tokens]\n",
    "        tokens_generated = []\n",
    "\n",
    "        while len(tokens_generated) <= self.max_tokens and tokens_generated[-1] != SPECIAL_TOKENS[END_OF_RECIPE]:\n",
    "            pad_len = MAX_LEN - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:MAX_LEN]\n",
    "                sample_index = MAX_LEN - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "        \n",
    "        generated_text = tokenizer.decode(self.start_tokens + tokens_generated)\n",
    "        print(f\"Generated text on epoch {epoch + 1}:\\n{generated_text}\")\n",
    "        try:\n",
    "            formatted_recipe = json.loads(generated_text)\n",
    "            print(json.dumps(formatted_recipe, indent=3))\n",
    "        except:\n",
    "            print(\"Something went wrong, failed to parse recipe into a JSON object.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "as_list() is not defined on an unknown TensorShape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m recipe_gen_callback \u001b[38;5;241m=\u001b[39m RecipeGenerator(start_tokens)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecipe_gen_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdrop_save_at_\u001b[39;49m\u001b[38;5;132;43;01m{epoch}\u001b[39;49;00m\u001b[38;5;124;43m.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ann_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/ann_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: as_list() is not defined on an unknown TensorShape."
     ]
    }
   ],
   "source": [
    "start_prompt = START_OF_RECIPE +'{\"ner\": [\"garlic\",'\n",
    "start_tokens = tokenizer.encode(start_prompt)\n",
    "recipe_gen_callback = RecipeGenerator(start_tokens)\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.fit(\n",
    "    dataset,\n",
    "    verbose=2,\n",
    "    epochs=25,\n",
    "    callbacks=[\n",
    "        recipe_gen_callback,\n",
    "        keras.callbacks.ModelCheckpoint(\"drop_save_at_{epoch}.keras\"),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
