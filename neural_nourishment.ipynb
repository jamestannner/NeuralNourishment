{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpgK_42V51IA"
      },
      "source": [
        "# Neural Nourishment\n",
        "\n",
        "This project draws on examples from the Keras tutorials [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and [GPT text generation from scratch with KerasNLP](https://keras.io/examples/generative/text_generation_gpt/), as well as the papers [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. and [\"Language Models are Few-Shot Listeners\"](https://arxiv.org/abs/2005.14165) by Brown et al.\n",
        "\n",
        "It uses [WordPiece Tokenization](https://research.google/blog/a-fast-wordpiece-tokenization-system/) and is trained on the [RecipeNLG dataset](https://www.kaggle.com/datasets/paultimothymooney/recipenlg) of 2,231,142 cooking recipes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vinre3Y351IB"
      },
      "source": [
        "**Import neccesary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-26T19:26:51.163936Z",
          "iopub.status.busy": "2024-04-26T19:26:51.163362Z",
          "iopub.status.idle": "2024-04-26T19:27:07.561659Z",
          "shell.execute_reply": "2024-04-26T19:27:07.560732Z",
          "shell.execute_reply.started": "2024-04-26T19:26:51.163908Z"
        },
        "id": "zyuopGJw51IB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "import keras_nlp\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings\n",
        "import tensorflow.io as tf_io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lPqzNvC9nuC",
        "outputId": "97ae02fe-9dfd-45b6-9251-605feb1eb499"
      },
      "outputs": [],
      "source": [
        "# train on TPU if appropriate\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "print(\"GPUS: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"GPU Available:\", tf.test.is_gpu_available())\n",
        "\n",
        "# Check TPU availability\n",
        "tpu_available = False\n",
        "devices = tf.config.list_logical_devices()\n",
        "for device in devices:\n",
        "    if device.device_type == 'TPU':\n",
        "        tpu_available = True\n",
        "        break\n",
        "\n",
        "print(\"TPU Available:\", tpu_available)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWcdHB2q51IB"
      },
      "source": [
        "---\n",
        "**Define model constants**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-26T19:27:32.702378Z",
          "iopub.status.busy": "2024-04-26T19:27:32.701669Z",
          "iopub.status.idle": "2024-04-26T19:27:32.71016Z",
          "shell.execute_reply": "2024-04-26T19:27:32.708986Z",
          "shell.execute_reply.started": "2024-04-26T19:27:32.702345Z"
        },
        "id": "NKb1h5K651IB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "BATCH_SIZE = 64 # Batch size we train on\n",
        "MIN_STRING_LEN = 512  # Strings shorter than this will be discarded\n",
        "SEQ_LEN = 512  # Length of training sequences, in tokens. AKA the context size\n",
        "\n",
        "# Model\n",
        "EMBED_DIM = 256 # size of each token's embedding\n",
        "FEED_FORWARD_DIM = 128 # feed forward network inside the transformer\n",
        "NUM_HEADS = 4 # number of attention heads\n",
        "NUM_LAYERS = 4 # number of transformers to stack\n",
        "VOCAB_SIZE = 2048  # token vocabulary size\n",
        "\n",
        "# Training\n",
        "EPOCHS = 40\n",
        "\n",
        "# Inference\n",
        "NUM_TOKENS_TO_GENERATE = 80\n",
        "\n",
        "# Special tokens\n",
        "START_OF_RECIPE = \"<|recipe_start|>\"\n",
        "END_OF_RECIPE = \"<|recipe_end|>\"\n",
        "PAD = \"<|pad|>\"\n",
        "OOV = \"<|oov|>\"\n",
        "SPECIAL_TOKENS = [PAD, START_OF_RECIPE, END_OF_RECIPE, OOV]\n",
        "\n",
        "# File names\n",
        "VOCAB_FILE = \"vocab.pickle\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj9SA17451IC"
      },
      "source": [
        "---\n",
        "**Define the dataset as strings of full recipes**\n",
        "\n",
        "To keep training managable for a laptop, we load the dataset into a tensorflow dataset object. This allows us to load data into memory as needed, opposed to all at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piQPjC5C51IC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def csv_row_to_json(row):\n",
        "    row = tf_io.decode_csv(records=row, record_defaults=[tf.constant([],dtype=tf.string)] * 7)\n",
        "\n",
        "    title = row[1]\n",
        "    ingredients = row[2]\n",
        "    directions = row[3]\n",
        "    ner = row[6]\n",
        "\n",
        "    return tf_strings.join([\n",
        "        '{\"ner\": ', ner, ', ',\n",
        "        '\"title\": \"', title, '\", ',\n",
        "        '\"ingredients\": ', ingredients, ', ',\n",
        "        '\"directions\": ', directions, '}',\n",
        "    ])\n",
        "\n",
        "\n",
        "dataset = (\n",
        "    tf_data.TextLineDataset(\"RecipeNLG/RecipeNLG_dataset.csv\") # load the csv file line by line\n",
        "    # tf_data.TextLineDataset(\"/kaggle/input/recipenlg/RecipeNLG_dataset.csv\") # load inside kaggle notebook\n",
        "    .skip(1) # skip the header row\n",
        "    .shuffle(buffer_size=256) # store 256 shuffled records in memory at a time before reshuffling and refetching\n",
        "    .map(lambda row: csv_row_to_json(row)) # map each row of the csv to a jsonified recipe\n",
        "    # .ignore_errors() # ignore any errors in the csv file\n",
        "    .apply(tf.data.experimental.ignore_errors()) # ignore any errors in the csv file\n",
        "    .batch(BATCH_SIZE) # batch the dataset to train on multiple records at once\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ESldOsq51IC"
      },
      "source": [
        "---\n",
        "**Tokenize the dataset**\n",
        "\n",
        "We train a WordPiece tokenizer on the dataset, reserving special tokens for the beginning and end of recipes. We can load the vocabulary and use the Keras `WordPieceTokenizer` to tokenize our tensors within the `tf.data` pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-26T19:27:59.712774Z",
          "iopub.status.busy": "2024-04-26T19:27:59.712312Z"
        },
        "id": "tTCJQHi851IC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# train the tokenizer's vocabulary\n",
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    data=dataset,\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    reserved_tokens=SPECIAL_TOKENS,\n",
        ")\n",
        "\n",
        "# save the vocabulary (so this step can be skipped in the future)\n",
        "with open(VOCAB_FILE, 'wb') as f:\n",
        "    pickle.dump(vocab, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "UnOaZNIl51IC",
        "outputId": "90ab9252-6b85-413f-9e0f-04bcf3bfebb9"
      },
      "outputs": [],
      "source": [
        "# load the vocabulary\n",
        "with open(VOCAB_FILE, \"rb\") as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "# load the tokenizer object with the trained vocbulary\n",
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    special_tokens_in_strings=True,\n",
        "    special_tokens=SPECIAL_TOKENS,\n",
        "    oov_token=OOV,\n",
        ")\n",
        "\n",
        "# Add start and end tokens, then pad the sequences\n",
        "packer = keras_nlp.layers.StartEndPacker(\n",
        "    sequence_length=SEQ_LEN,\n",
        "    start_value=tokenizer.token_to_id(START_OF_RECIPE),\n",
        "    end_value=tokenizer.token_to_id(END_OF_RECIPE),\n",
        "    pad_value=tokenizer.token_to_id(PAD),\n",
        ")\n",
        "\n",
        "def preprocess(recipe_batch):\n",
        "    outputs = tokenizer(recipe_batch)\n",
        "    features = packer(outputs)\n",
        "    labels = outputs\n",
        "    return features, labels\n",
        "\n",
        "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXKseIyU51IC"
      },
      "source": [
        "---\n",
        "**Define the model's architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iDBlcl351ID"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "    # token embedding layer\n",
        "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        vocabulary_size=VOCAB_SIZE,\n",
        "        sequence_length=SEQ_LEN,\n",
        "        embedding_dim=EMBED_DIM,\n",
        "        mask_zero=True,\n",
        "    )\n",
        "\n",
        "    # transformer decoders\n",
        "    transformer_layer = keras_nlp.layers.TransformerDecoder(\n",
        "        num_heads=NUM_HEADS,\n",
        "        intermediate_dim=FEED_FORWARD_DIM,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    # output layer\n",
        "    output_layer = keras.layers.Dense(VOCAB_SIZE)\n",
        "\n",
        "    # assemble the model\n",
        "    x = embedding_layer(inputs)\n",
        "    for _ in range(NUM_LAYERS): x = transformer_layer(x)\n",
        "    outputs = output_layer(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=loss_function, metrics=[perplexity])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "y1xJWyH551ID",
        "outputId": "4479c272-6fe7-4d51-86cb-7c5959283ef2"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_cJXCbq51ID"
      },
      "source": [
        "---\n",
        "**Define a custom callback for text generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX3jiQ6f51ID"
      },
      "outputs": [],
      "source": [
        "class TopKTextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, k):\n",
        "        self.sampler = keras_nlp.samplers.TopKSampler(k)\n",
        "        self.prompt_tokens = packer(tokenizer([START_OF_RECIPE]))\n",
        "\n",
        "    def _next(self, prompt, cache, index):\n",
        "        logits = model(prompt)[:, index-1, :]\n",
        "        hidden_states = None,\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        output_tokens = self.sampler(\n",
        "            next=self._next,\n",
        "            prompt=self.prompt_tokens,\n",
        "            index=1,\n",
        "        )\n",
        "        txt = tokenizer.detokenize(output_tokens)\n",
        "        print(f\"Top-K search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzaLSew751ID"
      },
      "source": [
        "---\n",
        "**Fit the model!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2Wvlj6h51ID",
        "outputId": "f4199e1b-398a-4ce3-a084-9f475053de06"
      },
      "outputs": [],
      "source": [
        "# training one epoch on my M1 macbook pro with 16GB of RAM takes roughly 5 hours 20 minutes ... :(\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='checkpoints/checkpoint_{epoch:02d}.keras',\n",
        "    save_best_only=False,\n",
        ")\n",
        "text_generation_callback = TopKTextGenerator(k=10)\n",
        "\n",
        "callbacks = [\n",
        "    checkpoint_callback,\n",
        "    # text_generation_callback,\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "notebook9003bbc4a7",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 1025978,
          "sourceId": 1728879,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30698,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
