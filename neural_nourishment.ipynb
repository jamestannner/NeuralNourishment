{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1728879,
          "sourceType": "datasetVersion",
          "datasetId": 1025978
        }
      ],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook9003bbc4a7",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Nourishment\n",
        "\n",
        "This project draws on examples from the Keras tutorials [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and [GPT text generation from scratch with KerasNLP](https://keras.io/examples/generative/text_generation_gpt/), as well as the papers [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. and [\"Language Models are Few-Shot Listeners\"](https://arxiv.org/abs/2005.14165) by Brown et al.\n",
        "\n",
        "It uses [WordPiece Tokenization](https://research.google/blog/a-fast-wordpiece-tokenization-system/) and is trained on the [RecipeNLG dataset](https://www.kaggle.com/datasets/paultimothymooney/recipenlg) of 2,231,142 cooking recipes.\n"
      ],
      "metadata": {
        "id": "IpgK_42V51IA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import neccesary libraries**"
      ],
      "metadata": {
        "id": "vinre3Y351IB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "import keras_nlp\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings\n",
        "import tensorflow.io as tf_io"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-26T19:26:51.163362Z",
          "iopub.execute_input": "2024-04-26T19:26:51.163936Z",
          "iopub.status.idle": "2024-04-26T19:27:07.561659Z",
          "shell.execute_reply.started": "2024-04-26T19:26:51.163908Z",
          "shell.execute_reply": "2024-04-26T19:27:07.560732Z"
        },
        "trusted": true,
        "id": "zyuopGJw51IB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train on TPU if appropriate\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "print(\"GPUS: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"GPU Available:\", tf.test.is_gpu_available())\n",
        "\n",
        "# Check TPU availability\n",
        "tpu_available = False\n",
        "devices = tf.config.list_logical_devices()\n",
        "for device in devices:\n",
        "    if device.device_type == 'TPU':\n",
        "        tpu_available = True\n",
        "        break\n",
        "\n",
        "print(\"TPU Available:\", tpu_available)\n"
      ],
      "metadata": {
        "id": "2lPqzNvC9nuC",
        "outputId": "97ae02fe-9dfd-45b6-9251-605feb1eb499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICAS:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-73e6a8cac211>:20: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUS:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "GPU Available: True\n",
            "TPU Available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Define model constants**"
      ],
      "metadata": {
        "id": "KWcdHB2q51IB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "BATCH_SIZE = 64 # Batch size we train on\n",
        "MIN_STRING_LEN = 512  # Strings shorter than this will be discarded\n",
        "SEQ_LEN = 128  # Length of training sequences, in tokens. AKA the context size\n",
        "\n",
        "# Model\n",
        "EMBED_DIM = 256 # size of each token's embedding\n",
        "FEED_FORWARD_DIM = 128 # feed forward network inside the transformer\n",
        "NUM_HEADS = 4 # number of attention heads\n",
        "NUM_LAYERS = 4 # number of transformers to stack\n",
        "VOCAB_SIZE = 2048  # token vocabulary size\n",
        "\n",
        "# Training\n",
        "EPOCHS = 40\n",
        "\n",
        "# Inference\n",
        "NUM_TOKENS_TO_GENERATE = 80\n",
        "\n",
        "# Special tokens\n",
        "START_OF_RECIPE = \"<|recipe_start|>\"\n",
        "END_OF_RECIPE = \"<|recipe_end|>\"\n",
        "PAD = \"<|pad|>\"\n",
        "OOV = \"<|oov|>\"\n",
        "SPECIAL_TOKENS = [PAD, START_OF_RECIPE, END_OF_RECIPE, OOV]\n",
        "\n",
        "# File names\n",
        "VOCAB_FILE = \"vocab.pickle\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-26T19:27:32.701669Z",
          "iopub.execute_input": "2024-04-26T19:27:32.702378Z",
          "iopub.status.idle": "2024-04-26T19:27:32.71016Z",
          "shell.execute_reply.started": "2024-04-26T19:27:32.702345Z",
          "shell.execute_reply": "2024-04-26T19:27:32.708986Z"
        },
        "trusted": true,
        "id": "NKb1h5K651IB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Define the dataset as strings of full recipes**\n",
        "\n",
        "To keep training managable for a laptop, we load the dataset into a tensorflow dataset object. This allows us to load data into memory as needed, opposed to all at once."
      ],
      "metadata": {
        "id": "Cj9SA17451IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_row_to_json(row):\n",
        "    row = tf_io.decode_csv(records=row, record_defaults=[tf.constant([],dtype=tf.string)] * 7)\n",
        "\n",
        "    title = row[1]\n",
        "    ingredients = row[2]\n",
        "    directions = row[3]\n",
        "    ner = row[6]\n",
        "\n",
        "    return tf_strings.join([\n",
        "        '{\"ner\": ', ner, ', ',\n",
        "        '\"title\": \"', title, '\", ',\n",
        "        '\"ingredients\": ', ingredients, ', ',\n",
        "        '\"directions\": ', directions, '}',\n",
        "    ])\n",
        "\n",
        "\n",
        "dataset = (\n",
        "#     tf_data.TextLineDataset(\"RecipeNLG/RecipeNLG_dataset.csv\") # load the csv file line by line\n",
        "    tf_data.TextLineDataset(\"/kaggle/input/recipenlg/RecipeNLG_dataset.csv\") # load inside kaggle notebook\n",
        "    .skip(1) # skip the header row\n",
        "    .shuffle(buffer_size=256) # store 256 shuffled records in memory at a time before reshuffling and refetching\n",
        "    .map(lambda row: csv_row_to_json(row)) # map each row of the csv to a jsonified recipe\n",
        "    .ignore_errors() # ignore any errors in the csv file\n",
        "    .batch(BATCH_SIZE) # batch the dataset to train on multiple records at once\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "piQPjC5C51IC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Tokenize the dataset**\n",
        "\n",
        "We train a WordPiece tokenizer on the dataset, reserving special tokens for the beginning and end of recipes. We can load the vocabulary and use the Keras `WordPieceTokenizer` to tokenize our tensors within the `tf.data` pipeline."
      ],
      "metadata": {
        "id": "5ESldOsq51IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the tokenizer's vocabulary\n",
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    data=dataset,\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    reserved_tokens=SPECIAL_TOKENS,\n",
        ")\n",
        "\n",
        "# save the vocabulary (so this step can be skipped in the future)\n",
        "with open(VOCAB_FILE, 'wb') as f:\n",
        "    pickle.dump(vocab, f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-26T19:27:59.712312Z",
          "iopub.execute_input": "2024-04-26T19:27:59.712774Z"
        },
        "trusted": true,
        "id": "tTCJQHi851IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the vocabulary\n",
        "with open(VOCAB_FILE, \"rb\") as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "# load the tokenizer object with the trained vocbulary\n",
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    special_tokens_in_strings=True,\n",
        "    special_tokens=SPECIAL_TOKENS,\n",
        "    oov_token=OOV,\n",
        ")\n",
        "\n",
        "# Add start and end tokens, then pad the sequences\n",
        "packer = keras_nlp.layers.StartEndPacker(\n",
        "    sequence_length=SEQ_LEN,\n",
        "    start_value=tokenizer.token_to_id(START_OF_RECIPE),\n",
        "    end_value=tokenizer.token_to_id(END_OF_RECIPE),\n",
        "    pad_value=tokenizer.token_to_id(PAD),\n",
        ")\n",
        "\n",
        "def preprocess(recipe_batch):\n",
        "    outputs = tokenizer(recipe_batch)\n",
        "    features = packer(outputs)\n",
        "    labels = outputs\n",
        "    return features, labels\n",
        "\n",
        "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "UnOaZNIl51IC",
        "outputId": "90ab9252-6b85-413f-9e0f-04bcf3bfebb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'vocab.pickle'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3f358248130e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# load the tokenizer object with the trained vocbulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vocab.pickle'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Define the model's architecture**"
      ],
      "metadata": {
        "id": "YXKseIyU51IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
        "\n",
        "    # token embedding layer\n",
        "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "        vocabulary_size=VOCAB_SIZE,\n",
        "        sequence_length=SEQ_LEN,\n",
        "        embedding_dim=EMBED_DIM,\n",
        "        mask_zero=True,\n",
        "    )\n",
        "\n",
        "    # transformer decoders\n",
        "    transformer_layer = keras_nlp.layers.TransformerDecoder(\n",
        "        num_heads=NUM_HEADS,\n",
        "        intermediate_dim=FEED_FORWARD_DIM,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    # output layer\n",
        "    output_layer = keras.layers.Dense(VOCAB_SIZE)\n",
        "\n",
        "    # assemble the model\n",
        "    x = embedding_layer(inputs)\n",
        "    for _ in range(NUM_LAYERS): x = transformer_layer(x)\n",
        "    outputs = output_layer(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=loss_function, metrics=[perplexity])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3iDBlcl351ID"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "y1xJWyH551ID",
        "outputId": "4479c272-6fe7-4d51-86cb-7c5959283ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m557,056\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbeddi…\u001b[0m │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │        \u001b[38;5;34m330,112\u001b[0m │ token_and_position_em… │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)      │                        │                │ transformer_decoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ transformer_decoder[\u001b[38;5;34m1\u001b[0m… │\n",
              "│                           │                        │                │ transformer_decoder[\u001b[38;5;34m2\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │        \u001b[38;5;34m526,336\u001b[0m │ transformer_decoder[\u001b[38;5;34m3\u001b[0m… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_and_position_embed… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">557,056</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbeddi…</span> │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">330,112</span> │ token_and_position_em… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)      │                        │                │ transformer_decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ transformer_decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>… │\n",
              "│                           │                        │                │ transformer_decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">526,336</span> │ transformer_decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>… │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,413,504\u001b[0m (5.39 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,413,504</span> (5.39 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,413,504\u001b[0m (5.39 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,413,504</span> (5.39 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Define a custom callback for text generation**"
      ],
      "metadata": {
        "id": "u_cJXCbq51ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKTextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, k):\n",
        "        self.sampler = keras_nlp.samplers.TopKSampler(k)\n",
        "        self.prompt_tokens = packer(tokenizer([START_OF_RECIPE]))\n",
        "\n",
        "    def _next(self, prompt, cache, index):\n",
        "        logits = model(prompt)[:, index-1, :]\n",
        "        hidden_states = None,\n",
        "        return logits, hidden_states, cache\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        output_tokens = self.sampler(\n",
        "            next=self._next,\n",
        "            prompt=self.prompt_tokens,\n",
        "            index=1,\n",
        "        )\n",
        "        txt = tokenizer.detokenize(output_tokens)\n",
        "        print(f\"Top-K search generated text: \\n{txt}\\n\")"
      ],
      "metadata": {
        "id": "tX3jiQ6f51ID"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Fit the model!**"
      ],
      "metadata": {
        "id": "uzaLSew751ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training one epoch on my M1 macbook pro with 16GB of RAM takes roughly 5 hours 20 minutes ... :(\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath='checkpoints/checkpoint_{epoch:02d}.keras',\n",
        "    save_best_only=False,\n",
        ")\n",
        "text_generation_callback = TopKTextGenerator(k=10)\n",
        "\n",
        "callbacks = [\n",
        "    checkpoint_callback,\n",
        "    text_generation_callback,\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    dataset.take,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        ")"
      ],
      "metadata": {
        "id": "z2Wvlj6h51ID",
        "outputId": "f4199e1b-398a-4ce3-a084-9f475053de06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:873: UserWarning: Layer 'position_embedding' (of type PositionEmbedding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:873: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:873: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:873: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      5/Unknown \u001b[1m30s\u001b[0m 4s/step - loss: 7.0109 - perplexity: 1280.6367"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-K search generated text: \n",
            "[b'<|recipe_start|> \" \" , \" \" , \" , \" cream \" \" cream , \" onion , \" : \" , onion \" \" , , \" , \" \" \" \" \" \" \" \"a \" \" \" \" \" \" \" \" \" \" \" \" , \" \" cream \" onion \" cream \" \" \" \" \" \"l \" \" \"l cream \" : \" \" \" \" \"l \" onion \" , \" onion \" S \" \" onion \" \" \" \" \" \" \" , \" \" \" cream \" \" \" S \"a \" \" \" S \" \" \" \" \" \" , \" \"a \" \" onion .a \" ,']\n",
            "\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 7s/step - loss: 6.8974 - perplexity: 1160.6100\n",
            "Epoch 2/2\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - loss: 4.6940 - perplexity: 110.9623Top-K search generated text: \n",
            "[b'<|recipe_start|> \" \" , \" , \" . \" : \" , \" , \" , \" : [ \" , \" , \" onion \" : \" \" , \" . , \" \" c , \" , \" , \" \" \" , \" , \" , \" , \" , \" , \" \" : water \" , \" , \" : \" , \" , \" , \" , \" \" , \" , \" , \" \" , \" \" , \" ,u , \" onion \" , \" , \" , \" , \" , \" , \" , \" , \" , \" 1 , \" , \" , \" , \" , sugar \" , \" , \" ,']\n",
            "\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 7s/step - loss: 4.6567 - perplexity: 107.0115\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7d3686a642e0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JzX_aaviBAdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}