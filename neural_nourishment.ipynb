{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nourishment\n",
    "\n",
    "This project draws on examples from the Keras tutorials [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and [GPT text generation from scratch with KerasNLP](https://keras.io/examples/generative/text_generation_gpt/), as well as the papers [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. and [\"Language Models are Few-Shot Listeners\"](https://arxiv.org/abs/2005.14165) by Brown et al.\n",
    "\n",
    "It uses [WordPiece Tokenization](https://research.google/blog/a-fast-wordpiece-tokenization-system/) and is trained on the [RecipeNLG dataset](https://www.kaggle.com/datasets/paultimothymooney/recipenlg) of 2,231,142 cooking recipes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import neccesary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "import tensorflow.io as tf_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define model constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "BATCH_SIZE = 64 # Batch size we train on\n",
    "MIN_STRING_LEN = 512  # Strings shorter than this will be discarded\n",
    "SEQ_LEN = 128  # Length of training sequences, in tokens. AKA the context size\n",
    "\n",
    "# Model\n",
    "EMBED_DIM = 256 # size of each token's embedding\n",
    "FEED_FORWARD_DIM = 128 # feed forward network inside the transformer\n",
    "NUM_HEADS = 3 # number of attention heads\n",
    "NUM_LAYERS = 2 # number of transformers to stack\n",
    "VOCAB_SIZE = 2048  # token vocabulary size\n",
    "\n",
    "# Training\n",
    "EPOCHS = 3\n",
    "\n",
    "# Inference\n",
    "NUM_TOKENS_TO_GENERATE = 80\n",
    "\n",
    "# Special tokens\n",
    "START_OF_RECIPE = \"<|recipe_start|>\"\n",
    "END_OF_RECIPE = \"<|recipe_end|>\"\n",
    "PAD = \"<|pad|>\"\n",
    "OOV = \"<|oov|>\"\n",
    "SPECIAL_TOKENS = [PAD, START_OF_RECIPE, END_OF_RECIPE, OOV]\n",
    "\n",
    "# File names\n",
    "VOCAB_FILE = \"vocab.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define the dataset as strings of full recipes**\n",
    "\n",
    "To keep training managable for a laptop, we load the dataset into a tensorflow dataset object. This allows us to load data into memory as needed, opposed to all at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_row_to_json(row):\n",
    "    row = tf_io.decode_csv(records=row, record_defaults=[tf.constant([],dtype=tf.string)] * 7)\n",
    "    \n",
    "    title = row[1]\n",
    "    ingredients = row[2]\n",
    "    directions = row[3]\n",
    "    ner = row[6]\n",
    "\n",
    "    return tf_strings.join([\n",
    "        '{\"ner\": ', ner, ', ',\n",
    "        '\"title\": \"', title, '\", ',\n",
    "        '\"ingredients\": ', ingredients, ', ',\n",
    "        '\"directions\": ', directions, '}',\n",
    "    ])\n",
    "\n",
    "dataset = (\n",
    "    tf_data.TextLineDataset(\"RecipeNLG/RecipeNLG_dataset.csv\") # load the csv file line by line\n",
    "    .skip(1) # skip the header row\n",
    "    .shuffle(buffer_size=256) # store 256 shuffled records in memory at a time before reshuffling and refetching\n",
    "    .map(lambda row: csv_row_to_json(row)) # map each row of the csv to a jsonified recipe\n",
    "    .apply(tf.data.experimental.ignore_errors()) # ignore any errors in the csv file\n",
    "    .batch(BATCH_SIZE) # batch the dataset to train on multiple records at once\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Tokenize the dataset**\n",
    "\n",
    "We train a WordPiece tokenizer on the dataset, reserving special tokens for the beginning and end of recipes. We can load the vocabulary and use the Keras `WordPieceTokenizer` to tokenize our tensors within the `tf.data` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the tokenizer's vocabulary\n",
    "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "    data=dataset,\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    reserved_tokens=SPECIAL_TOKENS,\n",
    ")\n",
    "\n",
    "# save the vocabulary (so this step can be skipped in the future)\n",
    "with open(VOCAB_FILE, 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "with open(VOCAB_FILE, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# load the tokenizer object with the trained vocbulary\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    special_tokens_in_strings=True,\n",
    "    special_tokens=SPECIAL_TOKENS,\n",
    "    oov_token=OOV,\n",
    ")\n",
    "\n",
    "# Add start and end tokens, then pad the sequences\n",
    "packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(START_OF_RECIPE),\n",
    "    end_value=tokenizer.token_to_id(END_OF_RECIPE),\n",
    "    pad_value=tokenizer.token_to_id(PAD),\n",
    ")\n",
    "\n",
    "def preprocess(recipe_batch):\n",
    "    outputs = tokenizer(recipe_batch)\n",
    "    features = packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define the model's architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "    # token embedding layer\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=VOCAB_SIZE,\n",
    "        sequence_length=SEQ_LEN,\n",
    "        embedding_dim=EMBED_DIM,\n",
    "        mask_zero=True,\n",
    "    )\n",
    "\n",
    "    # transformer decoders\n",
    "    transformer_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    # output layer\n",
    "    output_layer = keras.layers.Dense(VOCAB_SIZE)\n",
    "\n",
    "    # assemble the model\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(NUM_LAYERS): x = transformer_layer(x)\n",
    "    outputs = output_layer(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=loss_function, metrics=[perplexity])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define a custom callback for text generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, k):\n",
    "        self.sampler = keras_nlp.samplers.TopKSampler(k)\n",
    "        self.prompt_tokens = packer(tokenizer([START_OF_RECIPE]))\n",
    "\n",
    "    def _next(self, prompt, cache, index):\n",
    "        logits = model(prompt)[:, index-1, :]\n",
    "        hidden_states = None,\n",
    "        return logits, hidden_states, cache\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output_tokens = self.sampler(\n",
    "            next=self._next,\n",
    "            prompt=self.prompt_tokens,\n",
    "            index=1,\n",
    "        )\n",
    "        txt = tokenizer.detokenize(output_tokens)\n",
    "        print(f\"Top-K search generated text: \\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Fit the model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training one epoch on my M1 macbook pro with 16GB of RAM takes roughly 5 hours 20 minutes ... :(\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath='checkpoints/checkpoint_{epoch:02d}.tf')\n",
    "text_generation_callback = TopKTextGenerator(k=10)\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint_callback,\n",
    "    text_generation_callback,\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    dataset.take(5),\n",
    "    validation_data=dataset.skip(1).take(1),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
