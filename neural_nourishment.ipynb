{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nourishment\n",
    "\n",
    "This project draws on examples from the Keras tutorials [Text generation with a miniature GPT](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and [GPT text generation from scratch with KerasNLP](https://keras.io/examples/generative/text_generation_gpt/), as well as the papers [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al. and [\"Language Models are Few-Shot Listeners\"](https://arxiv.org/abs/2005.14165) by Brown et al.\n",
    "\n",
    "It uses [WordPiece Tokenization](https://research.google/blog/a-fast-wordpiece-tokenization-system/) and is trained on the [RecipeNLG dataset](https://www.kaggle.com/datasets/paultimothymooney/recipenlg) of 2,231,142 cooking recipes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import neccesary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "import tensorflow.io as tf_io\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define the dataset as strings of full recipes**\n",
    "\n",
    "To keep training managable for a laptop, we load the dataset into a tensorflow dataset object. This allows us to load data into memory as needed, opposed to all at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_row_to_json(row):\n",
    "    row = tf_io.decode_csv(records=row, record_defaults=[tf.constant([],dtype=tf.string)] * 7)\n",
    "    \n",
    "    title = row[1]\n",
    "    ingredients = row[2]\n",
    "    directions = row[3]\n",
    "    ner = row[6]\n",
    "\n",
    "    return tf_strings.join([\n",
    "        '{\"ner\": ', ner, ', ',\n",
    "        '\"title\": \"', title, '\", ',\n",
    "        '\"ingredients\": ', ingredients, ', ',\n",
    "        '\"directions\": ', directions, '}',\n",
    "    ])\n",
    "\n",
    "dataset = (\n",
    "    tf_data.TextLineDataset(\"RecipeNLG/RecipeNLG_dataset.csv\") # load the csv file line by line\n",
    "    .skip(1) # skip the header row\n",
    "    .shuffle(buffer_size=256) # store 256 shuffled records in memory at a time before reshuffling and refetching\n",
    "    .map(lambda row: csv_row_to_json(row)) # map each row of the csv to a jsonified recipe\n",
    "    .apply(tf.data.experimental.ignore_errors()) # ignore any errors in the csv file\n",
    "    .batch(BATCH_SIZE) # batch the dataset to train on multiple records at once\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Load the WordPiece tokenizer**\n",
    "\n",
    "In another notebook, we train a WordPiece tokenizer on the dataset. Special tokens are used for denoting the beginning and end of recipes. We can load the vocabulary and use the Keras `WordPieceTokenizer` to tokenize our tensors within the `tf.data` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "with open(VOCAB_FILE, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab,\n",
    "    sequence_length=SEQ_LEN,\n",
    "    special_tokens_in_strings=True,\n",
    "    special_tokens=SPECIAL_TOKENS,\n",
    "    oov_token=OOV,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Tokenize the dataset**\n",
    "\n",
    "Start and end tokens get added, then recipes are tokenized and prepared for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "packer = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LEN,\n",
    "    start_value=tokenizer.token_to_id(START_OF_RECIPE),\n",
    "    end_value=tokenizer.token_to_id(END_OF_RECIPE),\n",
    "    pad_value=tokenizer.token_to_id(PAD),\n",
    ")\n",
    "\n",
    "def preprocess(recipe_batch):\n",
    "    outputs = tokenizer(recipe_batch)\n",
    "    features = packer(outputs)\n",
    "    labels = outputs\n",
    "    return features, labels\n",
    "\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "    # token embedding layer\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "        vocabulary_size=VOCAB_SIZE,\n",
    "        sequence_length=SEQ_LEN,\n",
    "        embedding_dim=EMBED_DIM,\n",
    "        mask_zero=True,\n",
    "    )\n",
    "\n",
    "    # transformer decoders\n",
    "    transformer_layer = keras_nlp.layers.TransformerDecoder(\n",
    "        num_heads=NUM_HEADS,\n",
    "        intermediate_dim=FEED_FORWARD_DIM,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "    # output layer\n",
    "    output_layer = keras.layers.Dense(VOCAB_SIZE)\n",
    "\n",
    "    # assemble the model\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(NUM_LAYERS): x = transformer_layer(x)\n",
    "    outputs = output_layer(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=loss_function, metrics=[perplexity])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, None, 256)   557056      ['input_1[0][0]']                \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   329085      ['token_and_position_embedding[0]\n",
      " erDecoder)                                                      [0]',                            \n",
      "                                                                  'transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 2048)   526336      ['transformer_decoder[1][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,412,477\n",
      "Trainable params: 1,412,477\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Fit the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 18:16:07.793253: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "      5/Unknown - 4s 486ms/step - loss: 6.4108 - perplexity: 620.0692"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as token_embedding_layer_call_fn, token_embedding_layer_call_and_return_conditional_losses, position_embedding_layer_call_fn, position_embedding_layer_call_and_return_conditional_losses, self_attention_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/checkpoint_01.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/checkpoint_01.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 7s 1s/step - loss: 6.4108 - perplexity: 620.0692 - val_loss: 5.0042 - val_perplexity: 149.6012\n",
      "Epoch 2/3\n",
      "5/5 [==============================] - ETA: 0s - loss: 4.6304 - perplexity: 104.7357"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as token_embedding_layer_call_fn, token_embedding_layer_call_and_return_conditional_losses, position_embedding_layer_call_fn, position_embedding_layer_call_and_return_conditional_losses, self_attention_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/checkpoint_02.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/checkpoint_02.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 5s 1s/step - loss: 4.6304 - perplexity: 104.7357 - val_loss: 4.1266 - val_perplexity: 63.2959\n",
      "Epoch 3/3\n",
      "5/5 [==============================] - ETA: 0s - loss: 3.9786 - perplexity: 54.2793"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as token_embedding_layer_call_fn, token_embedding_layer_call_and_return_conditional_losses, position_embedding_layer_call_fn, position_embedding_layer_call_and_return_conditional_losses, self_attention_layer_call_fn while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/checkpoint_03.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/checkpoint_03.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 5s 1s/step - loss: 3.9786 - perplexity: 54.2793 - val_loss: 3.5556 - val_perplexity: 35.3025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16077fd30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training one epoch on my M1 macbook pro with 16GB of RAM takes roughly 5 hours 20 minutes ...\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath='checkpoints/checkpoint_{epoch:02d}.tf')\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint_callback,\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    dataset.take(5),\n",
    "    validation_data=dataset.skip(1).take(1),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
