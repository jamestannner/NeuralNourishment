{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import neccesary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define the `DEBUG` variable**\n",
    "\n",
    "Set `DEBUG` to `False` when you're ready to run and train the full model. To speed up debugging, we operate on a subset of the total dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Load the data and drop unwanted recipe sources**\n",
    "\n",
    "To keep training managable for a laptop, we drop some recipe sources from the dataset. We also reshuffle the recipes to ensure the model learns evenly from all remaining recipe sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size before: 2231142\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('RecipeNLG/RecipeNLG_dataset.csv')\n",
    "print(f'df size before: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size without cookbooks.com: 1000\n",
      "df size without allrecipes.com: 1000\n"
     ]
    }
   ],
   "source": [
    "df = df[~df['link'].str.contains('www.cookbooks.com')]\n",
    "print(f'df size without cookbooks.com: {len(df)}')\n",
    "df = df[~df['link'].str.contains('www.allrecipes.com')]\n",
    "print(f'df size without allrecipes.com: {len(df)}')\n",
    "df = df.sample(1000) if DEBUG else df.sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Transform the recipe dataset into a tokenization-ready string**\n",
    "\n",
    "Recipes are transformed into the string representation of a json object for easy manipulation once generated. Special tokens for denoting the start and end of a recipe are added to both ends of the stringified recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_OF_RECIPE = \"<|recipe_start|>\"\n",
    "END_OF_RECIPE = \"<|recipe_end|>\"\n",
    "\n",
    "def stringify_recipe(recipe):\n",
    "    title = recipe['title']\n",
    "    ingredients = eval(recipe['ingredients'])\n",
    "    directions = eval(recipe['directions'])\n",
    "    ner = eval(recipe['NER'])\n",
    "\n",
    "    stringified_recipe = json.dumps({\n",
    "        'ner': ner,\n",
    "        'title': title,\n",
    "        'ingredients': ingredients,\n",
    "        'directions': directions,\n",
    "    })\n",
    "    return START_OF_RECIPE + stringified_recipe + END_OF_RECIPE\n",
    "\n",
    "stringified_recipes = df.apply(stringify_recipe, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Tokenize the recipes**\n",
    "\n",
    "Using OpenAI's Base Pair Encoding tokenizer, [tiktoken](https://github.com/openai/tiktoken?tab=readme-ov-file), we tokenize the the recipe dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (315768,)\n",
      "[  100264  ,   5018  ,   1215  ,   794  ,   4482  ,   12440  ,   416  ,   498  ,   330  ,   337  ,   535  ,   5707  ,   498  ,   330  ,   566  ,   309  ,   23661  ,   498  ,   330  ,   273  ,   1677  ,   23661  ,   498  ,   330  ,   29468  ,   94582  ,   2857  ,   498  ,   330  ,   73480  ,   3258  ,   498  ,   330  ,   83  ,   1138  ,   6241  ,   498  ,   330  ,   72408  ,   523  ,   1924  ,   498  ,   330  ,   277  ,   773  ,   5724  ,   8073  ,   330  ,   2150  ,   794  ,   330  ,   1542  ,   1636  ,   2522  ,   37037  ,   1725  ,   449  ,   12093  ,   15386  ,   12279  ,   277  ,   416  ,   21252  ,   352  ,   263  ,   498  ,   330  ,   39220  ,   794  ,   4482  ,   17  ,   14971  ,   315  ,   31735  ,   498  ,   330  ,   19  ,   93200  ,   33213  ,   5707  ,   498  ,   330  ,   16  ,   220  ,   16  ,   14  ,   19  ,   16701  ,   9581  ,   24964  ,   54789  ,   477  ,   23542  ,   24964  ,   54789  ,   498  ,   330  ,   16  ,   14  ,   17  ,   10747  ,   72178  ,   60610  ,   23661  ,   498  ,   330  ,   18  ,   56588  ,   9235  ,   2807  ,   33780  ,   498  ,   330  ,   16  ,   62611  ,   7878  ,   30564  ,   23661  ,   498  ,   330  ,   19  ,   3544  ,   19151  ,   94582  ,   2857  ,   498  ,   330  ,   16  ,   42384  ,   38525  ,   7878  ,   88621  ,   498  ,   330  ,   16  ,   42384  ,   38525  ,   7878  ,   259  ,   1138  ,   6241  ,   498  ,   330  ,   16  ,   42384  ,   38525  ,   7878  ,   523  ,   1924  ,   498  ,   330  ,   18  ,   26446  ,   34504  ,   19937  ,   802  ,   773  ,   5724  ,   8073  ,   330  ,   19612  ,   82  ,   794  ,   4482  ,   4808  ,   20559  ,   24276  ,   311  ,   220  ,   3443  ,   37  ,   10684  ,   330  ,   38766  ,   1948  ,   220  ,   16  ,   14  ,   19  ,   17560  ,   1022  ,   14971  ,   315  ,   31735  ,   311  ,   29241  ,   85388  ,   10684  ,   330  ,   17826  ,   31735  ,   14971  ,   389  ,   3544  ,   11071  ,   315  ,   47499  ,   10684  ,   330  ,   9023  ,   44237  ,   220  ,   16  ,   42384  ,   5707  ,   927  ,   31735  ,   323  ,   15411  ,   63557  ,   719  ,   6724  ,   304  ,   47499  ,   26  ,   2035  ,   10361  ,   6089  ,   389  ,   24276  ,   30759  ,   323  ,   58785  ,   3156  ,   1633  ,   8579  ,   11  ,   922  ,   220  ,   1774  ,   4520  ,   10684  ,   330  ,   57850  ,   10684  ,   330  ,   50  ,   19901  ,   31735  ,   505  ,   51050  ,   1139  ,   19763  ,   10684  ,   330  ,   16834  ,   23243  ,   11  ,   63558  ,   54662  ,   31735  ,   3156  ,   11113  ,   10684  ,   330  ,   62406  ,   220  ,   18  ,   93200  ,   5707  ,   304  ,   8987  ,   3544  ,   79085  ,   927  ,   11298  ,   28661  ,   8798  ,   10684  ,   330  ,   64505  ,   36244  ,   24964  ,   54789  ,   449  ,   12290  ,   323  ,   25349  ,   10684  ,   330  ,   2520  ,   9581  ,   24964  ,   54789  ,   25  ,   328  ,   686  ,   4376  ,   520  ,   264  ,   892  ,   3156  ,   14198  ,   323  ,   1120  ,   47584  ,   304  ,   4219  ,   11  ,   922  ,   220  ,   16  ,   9568  ,   824  ,   3185  ,   10684  ,   330  ,   2520  ,   23542  ,   24964  ,   54789  ,   25  ,   16233  ,   1088  ,   4376  ,   520  ,   264  ,   892  ,   3156  ,   47584  ,   11  ,   55054  ,   14134  ,   11  ,   922  ,   220  ,   16  ,   220  ,   16  ,   14  ,   17  ,   4520  ,   824  ,   7309  ,   10684  ,   330  ,   22737  ,   311  ,   12235  ,   10684  ,   330  ,   51  ,   306  ,   449  ,   47499  ,   311  ,   2567  ,   8369  ,   10684  ,   330  ,   26054  ,   11  ,   4546  ,   60610  ,   23661  ,   323  ,   2807  ,   33780  ,   311  ,   44790  ,   304  ,   2678  ,   19737  ,   857  ,   26  ,   4148  ,   505  ,   8798  ,   10684  ,   330  ,   626  ,   404  ,   304  ,   30564  ,   23661  ,   10684  ,   330  ,   1671  ,   3267  ,   19151  ,   94582  ,   2857  ,   323  ,   220  ,   17  ,   56588  ,   54662  ,   31735  ,   10748  ,   68  ,   304  ,   3544  ,   9501  ,   19763  ,   311  ,   20955  ,   10684  ,   330  ,   24956  ,   1870  ,   41759  ,   304  ,   60610  ,   23661  ,   21655  ,   10684  ,   330  ,   17826  ,   19763  ,   927  ,   19737  ,   857  ,   315  ,   62915  ,   287  ,   3090  ,   320  ,   3055  ,   539  ,   2187  ,   19763  ,   311  ,   5916  ,   3090  ,   8  ,   323  ,   41759  ,   3156  ,   19972  ,   352  ,   263  ,   374  ,   12314  ,   323  ,   47985  ,   323  ,   93297  ,   25771  ,   220  ,   6330  ,   37  ,   11  ,   922  ,   220  ,   18  ,   4520  ,   10684  ,   330  ,   13319  ,   19763  ,   505  ,   927  ,   3090  ,   10684  ,   330  ,   1671  ,   3267  ,   304  ,   88621  ,   11  ,   259  ,   1138  ,   6241  ,   11  ,   323  ,   523  ,   1924  ,   10684  ,   330  ,   35960  ,   19972  ,   352  ,   263  ,   449  ,   12290  ,   323  ,   25349  ,   10684  ,   330  ,   12792  ,   579  ,   802  ,   773  ,   5724  ,   4315  ,   220  ,   19  ,   8369  ,   25485  ,   26  ,   2035  ,   24964  ,   54789  ,   47088  ,   802  ,   773  ,   5724  ,   10684  ,   330  ,   50  ,   33076  ,   19972  ,   352  ,   263  ,   927  ,   24964  ,   54789  ,   323  ,   8854  ,   1210  ,   14316  ,   100265  ,   100264  ,   5018  ,   1215  ,   794  ,   4482  ,   89945  ,   12932  ,   498  ,   330  ,   1517  ,   414  ,   498  ,   330  ,   82  ,   8734  ,   498  ,   330  ,   76  ,   34263  ,   498  ,   330  ,   29468  ,   498  ,   330  ,   273  ,   1677  ,   23661  ,   498  ,   330  ,   273  ,   1677  ,   81631  ,   498  ,   330  ,   11099  ,   62334  ,   14432  ,   498  ,   330  ,   82  ,   8734  ,   498  ,   330  ,   16023  ,   6374  ,   8073  ,   330  ,   2150  ,   794  ,   330  ,   91641  ,   1441  ,   264  ,   1208  ,   356  ,   9831  ,   498  ,   330  ,   39220  ,   794  ,   4482  ,   17  ,   220  ,   16  ,   14  ,   18  ,   26446  ,   8987  ,   12932  ,   498  ,   330  ,   16  ,   10747  ,   682  ,   59338  ,   20415  ,   498  ,   330  ,   16  ,   14  ,   17  ,   10747  ,   5636  ,   220  ,   16  ,   62611  ,   13465  ,   498  ,   330  ,   16  ,   14  ,   18  ,   10747  ,   14403  ,   498  ,   330  ,   16  ,   3544  ,   19151  ,   11  ,   34504  ,   31394  ,   498  ,   330  ,   16  ,   62611  ,   7878  ,   30564  ,   23661  ,   498  ,   330  ,   16  ,   42384  ,   61802  ,   97276  ,   30564  ,   81631  ,   498  ,   330  ,   19  ,   220  ,   16  ,   14  ,   17  ,   56588  ,   7120  ,   62334  ,   14432  ,   11  ,   50459  ,   323  ,   65410  ,   498  ,   330  ,   17  ,   56588  ,   390  ,   13421  ,   388  ,   6  ,   13465  ,   498  ,   330  ,   17  ,   93200  ,   10748  ,   33165  ,   8819  ,   8073  ,   330  ,   19612  ,   82  ,   794  ,   4482  ,   644  ,   264  ,   11298  ,   19763  ,   11  ,   16343  ,   220  ,   16  ,   14  ,   18  ,   10747  ,   315  ,   279  ,   12932  ,   449  ,   279  ,   20415  ,   11  ,   13465  ,   11  ,   14403  ,   11  ,   19151  ,   11  ,   30564  ,   23661  ,   11  ,   30564  ,   81631  ,   323  ,   220  ,   17  ,   220  ,   16  ,   14  ,   17  ,   56588  ,   315  ,   279  ,   14432  ,   10684  ,   330  ,   44758  ,   3156  ,   279  ,   8919  ,   374  ,   11113  ,   10684  ,   330  ,   4808  ,   20559  ,   264  ,   281  ,   8934  ,   6853  ,   11245  ,   927  ,   70351  ,   3428  ,   8798  ,   10684  ,   330  ,   14235  ,   398  ,   15998  ,   279  ,   11245  ,   449  ,   1063  ,   315  ,   279  ,   9861  ,   14432  ,   323  ,   2035  ,   264  ,   568  ,   14550  ,   62611  ,   315  ,   279  ,   8919  ,   304  ,   279  ,   4219  ,   10684  ,   330  ,   8084  ,   279  ,   11245  ,   323  ,   37663  ,   279  ,   13777  ,   369  ,   264  ,   2478  ,   6622  ,   10684  ,   330  ,   16464  ,   279  ,   7410  ,   323  ,   4394  ,   369  ,   922  ,   220  ,   966  ,   6622  ,   477  ,   3156  ,   279  ,   10667  ,   809  ,   4131  ,   3201  ,   505  ,   832  ,   3185  ,   315  ,   279  ,   11245  ,   10684  ,   330  ,   47908  ,   323  ,   23360  ,   279  ,   1023  ,   3185  ,   3156  ,   21411  ,   11  ,   220  ,   508  ,   311  ,   220  ,   966  ,   6622  ,   5129  ,   10684  ,   330  ,   16834  ,   264  ,   22145  ,   11813  ,   11  ,   12157  ,   279  ,   6964  ,   315  ,   279  ,   281  ,   8934  ,   6853  ,   3201  ,   505  ,   279  ,   11245  ,   26  ,   15884  ,   58212  ,   279  ,   10667  ,   809  ,   1022  ,   279  ,   11245  ,   323  ,   6638  ,   433  ,   2212  ,   264  ,   3544  ,   74155  ,   11813  ,   477  ,   14068  ,   14559  ,   29561  ,   311  ,   1376  ,   264  ,   22949  ,   10684  ,   330  ,   39818  ,   311  ,   1376  ,   279  ,   9861  ,   87992  ,   11  ,   14432  ,   287  ,   279  ,   281  ,   8934  ,   6853  ,   11245  ,   323  ,   43468  ,   279  ,   8798  ,   439  ,   5995  ,   10684  ,   330  ,   644  ,   264  ,   3544  ,   19763  ,   11  ,   51176  ,   279  ,   9861  ,   220  ,   17  ,   26446  ,   315  ,   12932  ,   449  ,   279  ,   390  ,   13421  ,   388  ,   6  ,   13465  ,   323  ,   33165  ,   10684  ,   330  ,   50  ,   33076  ,   477  ,   13961  ,   279  ,   12932  ,   1139  ,   279  ,   87992  ,   323  ,   8854  ,   1210  ,   14316  ,   100265  ,   100264  ,   5018  ,   1215  ,   794  ,   4482  ,   21946  ,   84  ,   6672  ,   498  ,   330  ,   337  ,   535  ,   5707  ,   8073  ,   330  ,   2150  ,   794  ,   330  ,   34  ,   42602  ,   6729  ,   9806  ,   82  ,   498  ,   330  ,   39220  ,   794  ,   4482  ,   16  ,   11298  ,   9145  ,   84  ,   6672  ,   11  ,   4018  ,   1139  ,   27219  ,   38502  ,   220  ,   16  ,   14  ,   19  ,   482  ,   37850  ,   7716  ,   875  ,   35354  ,   498  ,   330  ,   10714  ,   220  ,   16  ,   14  ,   19  ,   10747  ,   33213  ,   5707  ,   11  ,   369  ,   77826  ,   8073  ,   330  ,   19612  ,   82  ,   794  ,   4482  ,   4808  ,   20559  ,   279  ,   24276  ,   311  ,   220  ,   8652  ,   10684  ,   330  ,   80335  ,   279  ,   16385  ,   389  ,   220  ,   17  ,   3544  ,   28915  ,   25112  ,   323  ,   15998  , ]\n",
      "[  <|recipe_start|>  ,   {\"  ,   ner  ,   \":  ,    [\"  ,   gar  ,   lic  ,   \",  ,    \"  ,   ol  ,   ive  ,    oil  ,   \",  ,    \"  ,   cl  ,   am  ,    juice  ,   \",  ,    \"  ,   le  ,   mon  ,    juice  ,   \",  ,    \"  ,   egg  ,    yol  ,   ks  ,   \",  ,    \"  ,   pars  ,   ley  ,   \",  ,    \"  ,   t  ,   arr  ,   agon  ,   \",  ,    \"  ,   fresh  ,    ch  ,   ives  ,   \",  ,    \"  ,   ar  ,   ug  ,   ula  ,   \"],  ,    \"  ,   title  ,   \":  ,    \"  ,   Se  ,   ared  ,    Sc  ,   allo  ,   ps  ,    with  ,    Ro  ,   asted  ,   -G  ,   ar  ,   lic  ,    Sab  ,   ay  ,   on  ,   \",  ,    \"  ,   ingredients  ,   \":  ,    [\"  ,   2  ,    heads  ,    of  ,    garlic  ,   \",  ,    \"  ,   4  ,    teaspoons  ,    olive  ,    oil  ,   \",  ,    \"  ,   1  ,      ,   1  ,   /  ,   4  ,    pounds  ,    sea  ,    scal  ,   lops  ,    or  ,    bay  ,    scal  ,   lops  ,   \",  ,    \"  ,   1  ,   /  ,   2  ,    cup  ,    bottled  ,    clam  ,    juice  ,   \",  ,    \"  ,   3  ,    tablespoons  ,    dry  ,    ver  ,   mouth  ,   \",  ,    \"  ,   1  ,    tablespoon  ,    fresh  ,    lemon  ,    juice  ,   \",  ,    \"  ,   4  ,    large  ,    egg  ,    yol  ,   ks  ,   \",  ,    \"  ,   1  ,    teaspoon  ,    chopped  ,    fresh  ,    parsley  ,   \",  ,    \"  ,   1  ,    teaspoon  ,    chopped  ,    fresh  ,    t  ,   arr  ,   agon  ,   \",  ,    \"  ,   1  ,    teaspoon  ,    chopped  ,    fresh  ,    ch  ,   ives  ,   \",  ,    \"  ,   3  ,    cups  ,    lightly  ,    packed  ,    ar  ,   ug  ,   ula  ,   \"],  ,    \"  ,   direction  ,   s  ,   \":  ,    [\"  ,   Pre  ,   heat  ,    oven  ,    to  ,      ,   400  ,   F  ,   .\",  ,    \"  ,   Cut  ,    top  ,      ,   1  ,   /  ,   4  ,    inch  ,    off  ,    heads  ,    of  ,    garlic  ,    to  ,    expose  ,    cloves  ,   .\",  ,    \"  ,   Place  ,    garlic  ,    heads  ,    on  ,    large  ,    sheet  ,    of  ,    foil  ,   .\",  ,    \"  ,   Dr  ,   izzle  ,      ,   1  ,    teaspoon  ,    oil  ,    over  ,    garlic  ,    and  ,    wrap  ,    loosely  ,    but  ,    completely  ,    in  ,    foil  ,   ;  ,    place  ,    packet  ,    directly  ,    on  ,    oven  ,    rack  ,    and  ,    roast  ,    until  ,    very  ,    soft  ,   ,  ,    about  ,      ,   45  ,    minutes  ,   .\",  ,    \"  ,   Cool  ,   .\",  ,    \"  ,   S  ,   queeze  ,    garlic  ,    from  ,    skins  ,    into  ,    bowl  ,   .\",  ,    \"  ,   Using  ,    fork  ,   ,  ,    mash  ,    roasted  ,    garlic  ,    until  ,    smooth  ,   .\",  ,    \"  ,   Heat  ,      ,   3  ,    teaspoons  ,    oil  ,    in  ,    heavy  ,    large  ,    skillet  ,    over  ,    medium  ,   -high  ,    heat  ,   .\",  ,    \"  ,   Spr  ,   inkle  ,    scal  ,   lops  ,    with  ,    salt  ,    and  ,    pepper  ,   .\",  ,    \"  ,   For  ,    sea  ,    scal  ,   lops  ,   :  ,    S  ,   ear  ,    half  ,    at  ,    a  ,    time  ,    until  ,    brown  ,    and  ,    just  ,    opaque  ,    in  ,    center  ,   ,  ,    about  ,      ,   1  ,    minute  ,    per  ,    side  ,   .\",  ,    \"  ,   For  ,    bay  ,    scal  ,   lops  ,   :  ,    Sa  ,   ute  ,    half  ,    at  ,    a  ,    time  ,    until  ,    opaque  ,   ,  ,    stirring  ,    frequently  ,   ,  ,    about  ,      ,   1  ,      ,   1  ,   /  ,   2  ,    minutes  ,    per  ,    batch  ,   .\",  ,    \"  ,   Transfer  ,    to  ,    plate  ,   .\",  ,    \"  ,   T  ,   ent  ,    with  ,    foil  ,    to  ,    keep  ,    warm  ,   .\",  ,    \"  ,   Meanwhile  ,   ,  ,    bring  ,    clam  ,    juice  ,    and  ,    ver  ,   mouth  ,    to  ,    boil  ,    in  ,    small  ,    sauce  ,   pan  ,   ;  ,    remove  ,    from  ,    heat  ,   .\",  ,    \"  ,   St  ,   ir  ,    in  ,    lemon  ,    juice  ,   .\",  ,    \"  ,   Wh  ,   isk  ,    egg  ,    yol  ,   ks  ,    and  ,      ,   2  ,    tablespoons  ,    roasted  ,    garlic  ,    pure  ,   e  ,    in  ,    large  ,    metal  ,    bowl  ,    to  ,    blend  ,   .\",  ,    \"  ,   Grad  ,   ually  ,    whisk  ,    in  ,    clam  ,    juice  ,    mixture  ,   .\",  ,    \"  ,   Place  ,    bowl  ,    over  ,    sauce  ,   pan  ,    of  ,    simmer  ,   ing  ,    water  ,    (  ,   do  ,    not  ,    allow  ,    bowl  ,    to  ,    touch  ,    water  ,   )  ,    and  ,    whisk  ,    until  ,    sab  ,   ay  ,   on  ,    is  ,    thick  ,    and  ,    creamy  ,    and  ,    thermometer  ,    registers  ,      ,   160  ,   F  ,   ,  ,    about  ,      ,   3  ,    minutes  ,   .\",  ,    \"  ,   Remove  ,    bowl  ,    from  ,    over  ,    water  ,   .\",  ,    \"  ,   Wh  ,   isk  ,    in  ,    parsley  ,   ,  ,    t  ,   arr  ,   agon  ,   ,  ,    and  ,    ch  ,   ives  ,   .\",  ,    \"  ,   Season  ,    sab  ,   ay  ,   on  ,    with  ,    salt  ,    and  ,    pepper  ,   .\",  ,    \"  ,   Div  ,   ide  ,    ar  ,   ug  ,   ula  ,    among  ,      ,   4  ,    warm  ,    plates  ,   ;  ,    place  ,    scal  ,   lops  ,    atop  ,    ar  ,   ug  ,   ula  ,   .\",  ,    \"  ,   S  ,   poon  ,    sab  ,   ay  ,   on  ,    over  ,    scal  ,   lops  ,    and  ,    serve  ,   .\"  ,   ]}  ,   <|recipe_end|>  ,   <|recipe_start|>  ,   {\"  ,   ner  ,   \":  ,    [\"  ,   heavy  ,    cream  ,   \",  ,    \"  ,   fl  ,   our  ,   \",  ,    \"  ,   s  ,   ugar  ,   \",  ,    \"  ,   m  ,   ilk  ,   \",  ,    \"  ,   egg  ,   \",  ,    \"  ,   le  ,   mon  ,    juice  ,   \",  ,    \"  ,   le  ,   mon  ,    zest  ,   \",  ,    \"  ,   uns  ,   alted  ,    butter  ,   \",  ,    \"  ,   s  ,   ugar  ,   \",  ,    \"  ,   van  ,   illa  ,   \"],  ,    \"  ,   title  ,   \":  ,    \"  ,   Corn  ,   ets  ,    a  ,    la  ,    C  ,   reme  ,   \",  ,    \"  ,   ingredients  ,   \":  ,    [\"  ,   2  ,      ,   1  ,   /  ,   3  ,    cups  ,    heavy  ,    cream  ,   \",  ,    \"  ,   1  ,    cup  ,    all  ,   -purpose  ,    flour  ,   \",  ,    \"  ,   1  ,   /  ,   2  ,    cup  ,    plus  ,      ,   1  ,    tablespoon  ,    sugar  ,   \",  ,    \"  ,   1  ,   /  ,   3  ,    cup  ,    milk  ,   \",  ,    \"  ,   1  ,    large  ,    egg  ,   ,  ,    lightly  ,    beaten  ,   \",  ,    \"  ,   1  ,    tablespoon  ,    fresh  ,    lemon  ,    juice  ,   \",  ,    \"  ,   1  ,    teaspoon  ,    finely  ,    grated  ,    lemon  ,    zest  ,   \",  ,    \"  ,   4  ,      ,   1  ,   /  ,   2  ,    tablespoons  ,    uns  ,   alted  ,    butter  ,   ,  ,    melted  ,    and  ,    cooled  ,   \",  ,    \"  ,   2  ,    tablespoons  ,    con  ,   fection  ,   ers  ,   '  ,    sugar  ,   \",  ,    \"  ,   2  ,    teaspoons  ,    pure  ,    vanilla  ,    extract  ,   \"],  ,    \"  ,   direction  ,   s  ,   \":  ,    [\"  ,   In  ,    a  ,    medium  ,    bowl  ,   ,  ,    combine  ,      ,   1  ,   /  ,   3  ,    cup  ,    of  ,    the  ,    cream  ,    with  ,    the  ,    flour  ,   ,  ,    sugar  ,   ,  ,    milk  ,   ,  ,    egg  ,   ,  ,    lemon  ,    juice  ,   ,  ,    lemon  ,    zest  ,    and  ,      ,   2  ,      ,   1  ,   /  ,   2  ,    tablespoons  ,    of  ,    the  ,    butter  ,   .\",  ,    \"  ,   Beat  ,    until  ,    the  ,    batter  ,    is  ,    smooth  ,   .\",  ,    \"  ,   Pre  ,   heat  ,    a  ,    p  ,   izz  ,   elle  ,    iron  ,    over  ,    moderately  ,    low  ,    heat  ,   .\",  ,    \"  ,   Light  ,   ly  ,    brush  ,    the  ,    iron  ,    with  ,    some  ,    of  ,    the  ,    remaining  ,    butter  ,    and  ,    place  ,    a  ,    he  ,   aping  ,    tablespoon  ,    of  ,    the  ,    batter  ,    in  ,    the  ,    center  ,   .\",  ,    \"  ,   Close  ,    the  ,    iron  ,    and  ,    squeeze  ,    the  ,    handles  ,    for  ,    a  ,    few  ,    seconds  ,   .\",  ,    \"  ,   Release  ,    the  ,    pressure  ,    and  ,    cook  ,    for  ,    about  ,      ,   30  ,    seconds  ,    or  ,    until  ,    the  ,    wa  ,   fer  ,    comes  ,    away  ,    from  ,    one  ,    side  ,    of  ,    the  ,    iron  ,   .\",  ,    \"  ,   Flip  ,    and  ,    bake  ,    the  ,    other  ,    side  ,    until  ,    golden  ,   ,  ,      ,   20  ,    to  ,      ,   30  ,    seconds  ,    longer  ,   .\",  ,    \"  ,   Using  ,    a  ,    knife  ,    tip  ,   ,  ,    lift  ,    the  ,    edge  ,    of  ,    the  ,    p  ,   izz  ,   elle  ,    away  ,    from  ,    the  ,    iron  ,   ;  ,    carefully  ,    peel  ,    the  ,    wa  ,   fer  ,    off  ,    the  ,    iron  ,    and  ,    roll  ,    it  ,    around  ,    a  ,    large  ,    pastry  ,    tip  ,    or  ,    cann  ,   oli  ,    mold  ,    to  ,    form  ,    a  ,    cone  ,   .\",  ,    \"  ,   Repeat  ,    to  ,    form  ,    the  ,    remaining  ,    cones  ,   ,  ,    butter  ,   ing  ,    the  ,    p  ,   izz  ,   elle  ,    iron  ,    and  ,    adjusting  ,    the  ,    heat  ,    as  ,    necessary  ,   .\",  ,    \"  ,   In  ,    a  ,    large  ,    bowl  ,   ,  ,    whip  ,    the  ,    remaining  ,      ,   2  ,    cups  ,    of  ,    cream  ,    with  ,    the  ,    con  ,   fection  ,   ers  ,   '  ,    sugar  ,    and  ,    vanilla  ,   .\",  ,    \"  ,   S  ,   poon  ,    or  ,    pipe  ,    the  ,    cream  ,    into  ,    the  ,    cones  ,    and  ,    serve  ,   .\"  ,   ]}  ,   <|recipe_end|>  ,   <|recipe_start|>  ,   {\"  ,   ner  ,   \":  ,    [\"  ,   bag  ,   u  ,   ette  ,   \",  ,    \"  ,   ol  ,   ive  ,    oil  ,   \"],  ,    \"  ,   title  ,   \":  ,    \"  ,   C  ,   rost  ,   ini  ,    Toast  ,   s  ,   \",  ,    \"  ,   ingredients  ,   \":  ,    [\"  ,   1  ,    medium  ,    bag  ,   u  ,   ette  ,   ,  ,    cut  ,    into  ,    thirty  ,   -two  ,      ,   1  ,   /  ,   4  ,    -  ,   inch  ,   -th  ,   ick  ,    slices  ,   \",  ,    \"  ,   About  ,      ,   1  ,   /  ,   4  ,    cup  ,    olive  ,    oil  ,   ,  ,    for  ,    brushing  ,   \"],  ,    \"  ,   direction  ,   s  ,   \":  ,    [\"  ,   Pre  ,   heat  ,    the  ,    oven  ,    to  ,      ,   350  ,   .\",  ,    \"  ,   Arrange  ,    the  ,    bread  ,    on  ,      ,   2  ,    large  ,    baking  ,    sheets  ,    and  ,    brush  , ]\n"
     ]
    }
   ],
   "source": [
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "    START_OF_RECIPE: 100264,\n",
    "    END_OF_RECIPE: 100265,\n",
    "}\n",
    "\n",
    "enc = tiktoken.Encoding(\n",
    "    name=\"cl100k_im\",\n",
    "    pat_str=cl100k_base._pat_str,\n",
    "    mergeable_ranks=cl100k_base._mergeable_ranks,\n",
    "    special_tokens={\n",
    "        **cl100k_base._special_tokens,\n",
    "        **SPECIAL_TOKENS,\n",
    "    }\n",
    ")\n",
    "\n",
    "encoded_recipes = np.array([], dtype=np.int64)\n",
    "for recipe in stringified_recipes:\n",
    "    encoded_recipe = enc.encode(recipe, allowed_special=\"all\")\n",
    "    encoded_recipes = np.append(encoded_recipes, encoded_recipe)\n",
    "\n",
    "VOCAB_SIZE = len(set(encoded_recipes))\n",
    "\n",
    "print(type(encoded_recipes), encoded_recipes.shape)\n",
    "\n",
    "print(\"[\", end='')\n",
    "for token in encoded_recipes[:1000]:\n",
    "    print(f\"  {token}  , \", end='')\n",
    "print(\"]\")\n",
    "\n",
    "\n",
    "print(\"[\", end='')\n",
    "for token in encoded_recipes[:1000]:\n",
    "    print(f\"  {enc.decode([token])}  , \", end='')\n",
    "print(\"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Split into training and validation datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VAL_SPLIT = 0.9 # 90% training, 10% validation\n",
    "\n",
    "n = int(TRAIN_VAL_SPLIT * len(encoded_recipes))\n",
    "training_data = encoded_recipes[:n]\n",
    "validation_data = encoded_recipes[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define the data batching function**\n",
    "\n",
    "The `X` term is a random slice of length `CONTEXT_SIZE` from the specified data source (training or valdation). The `Y` term is the same but offset by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91178  3156 21411   389  1855]\n",
      "[ 3156 21411   389  1855  3185]\n",
      "with context=[91178] the target is 3156\n",
      "with context=[91178, 3156] the target is 21411\n",
      "with context=[91178, 3156, 21411] the target is 389\n",
      "with context=[91178, 3156, 21411, 389] the target is 1855\n",
      "with context=[91178, 3156, 21411, 389, 1855] the target is 3185\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 5\n",
    "\n",
    "def get_batch(split):\n",
    "    data = training_data if split == \"train\" else validation_data\n",
    "    start_i = np.random.randint(len(data) - CONTEXT_SIZE) \n",
    "\n",
    "    X = data[start_i:start_i+CONTEXT_SIZE]\n",
    "    y = data[start_i+1:start_i+CONTEXT_SIZE+1]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "Xb, yb = get_batch(\"train\")\n",
    "print(Xb)\n",
    "print(yb)\n",
    "\n",
    "for c in range(CONTEXT_SIZE):\n",
    "    context = Xb[:c+1]\n",
    "    target = yb[c]\n",
    "    print(f\"with context={context.tolist()} the target is {target}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Implement a TransformerBlock layer and a TokenAndPositionEmbedding layer.**\n",
    "\n",
    "Based on the examples from [this Keras tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = ops.arange(n_dest)[:, None]\n",
    "    j = ops.arange(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = ops.cast(m, dtype)\n",
    "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = ops.concatenate(\n",
    "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return ops.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(0, maxlen, 1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Define the model architecture and hyperparameters**\n",
    "\n",
    "Again, based on the examples from [this Keras tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) and the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) by Vaswani et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 80  # Max sequence size\n",
    "EMBEDDING_SIZE = 256\n",
    "NUM_ATTENTION_HEADS = 2\n",
    "FEED_FORWARD_SIZE = 256\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(EMBEDDING_SIZE, NUM_ATTENTION_HEADS, FEED_FORWARD_SIZE)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(VOCAB_SIZE)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\",\n",
    "        loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define our RecipeGenerator class**\n",
    "\n",
    "Inspired by the example in [the Keras tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) but adapted to fit this use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecipeGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate a recipe from our trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input \n",
    "\n",
    "    Arguments:\n",
    "        start_tokens: List of integers, the tokens for the starting prompt.\n",
    "        top_k: Integer, sample from the `top_k` token predictions. Defaults to 10.\n",
    "        print_every: Integer, print after this many epochs. Defaults to 1.\n",
    "        max_tokens: Integer, the maximum number of tokens to be generated after prompt. \n",
    "            Generation will end early when we reach the `<|recipe_end|>` token. Defaults to 300.\n",
    "    \"\"\"\n",
    "    def __init__(self, start_tokens, top_k=10, print_every=1, max_tokens=300):\n",
    "        self.start_tokens = start_tokens\n",
    "        self.top_k = top_k\n",
    "        self.print_every = print_every\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, idxs = ops.top_k(logits, k=self.top_k, sorted=True)\n",
    "        idsx = np.asarray(idxs).astype(\"int32\")\n",
    "        predictions = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
    "        predictions = np.asarray(predictions).astype(\"float32\")\n",
    "        return np.random.choice(idxs, p=predictions)\n",
    "    \n",
    "    def on_epoch_end(self, epoch):\n",
    "        start_tokens = [token for token in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0: return\n",
    "\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "\n",
    "        while num_tokens_generated <= self.max_tokens and tokens_generated[-1] != SPECIAL_TOKENS[END_OF_RECIPE]:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        \n",
    "        return enc.decode(self.start_tokens + tokens_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
